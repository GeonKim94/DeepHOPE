{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def modify_numpy_array(array):\n",
    "    array += 10\n",
    "    return array\n",
    "\n",
    "# Creating a NumPy array\n",
    "my_array = np.array([1, 2, 3])\n",
    "\n",
    "# Modifying the array within the function\n",
    "modified_array = modify_numpy_array(my_array)\n",
    "\n",
    "# Both references point to the same modified array\n",
    "print(\"Original array:\", my_array)\n",
    "print(\"Modified array:\", modified_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "import time\n",
    "\n",
    "# Create a sample image\n",
    "image_size = (500, epochs0)\n",
    "image = np.random.rand(*image_size)\n",
    "\n",
    "# Time swapping axes with numpy.swapaxes\n",
    "start_time = time.time()\n",
    "swapped_image = np.swapaxes(image, 0, 1)\n",
    "swap_time = time.time() - start_time\n",
    "\n",
    "# Time rotating by 90 degrees with scipy.ndimage.interpolate.rotate\n",
    "start_time = time.time()\n",
    "rotated_image = ndimage.interpolation.rotate(image, angle=90, reshape=False)\n",
    "rotate_time = time.time() - start_time\n",
    "\n",
    "print(f\"Swapping axes time: {swap_time:.5f} seconds\")\n",
    "print(f\"Rotating 90 degrees time: {rotate_time:.5f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import h5py\n",
    "\n",
    "def check_h5_compression(file_path):\n",
    "    with h5py.File(file_path, 'r') as h5file:\n",
    "        for dataset_name in h5file:\n",
    "            dataset = h5file[dataset_name]\n",
    "\n",
    "            # Check if compression is applied\n",
    "            if 'compression' in dataset.attrs:\n",
    "                compression_type = dataset.attrs['compression']\n",
    "                print(f\"The dataset '{dataset_name}' has compression: {compression_type}\")\n",
    "            #else:\n",
    "            #    print(f\"The dataset '{dataset_name}' does not have compression.\")\n",
    "\n",
    "\n",
    "def check_compression_in_directory(root_dir, file_extension='.h5'):\n",
    "    h5_files = glob.glob(os.path.join(root_dir, f\"**/*{file_extension}\"), recursive=True)\n",
    "\n",
    "    for h5_file in h5_files:\n",
    "        check_h5_compression(h5_file)\n",
    "\n",
    "# Example usage\n",
    "directory_path = \"/data02/gkim/stem_cell_jwshin/data/23_SEC1H5_wider_v3_testiPSC/\"\n",
    "#directory_path = \"/data02/gkim/stem_cell_jwshin/data/23_SEC1H5_wider_v3_allh_onRA/\"\n",
    "check_compression_in_directory(directory_path)\n",
    "\n",
    "# Example usage\n",
    "#file_path = \"/data02/gkim/stem_cell_jwshin/data/23_SEC1H5_wider_v3_allh_onRA/train/00_RA/230922.234951.JAX_24hours.058.Group1.A1.S058.h5\"\n",
    "#check_h5_compression(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "\n",
    "def compress_and_copy_h5_files(source_dir, dest_dir, compression='gzip', compression_opts=9):\n",
    "    # Create the destination directory if it doesn't exist\n",
    "    if not os.path.exists(dest_dir):\n",
    "        os.makedirs(dest_dir)\n",
    "\n",
    "    # Iterate through all HDF5 files in the source directory\n",
    "    for root, dirs, files in os.walk(source_dir):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith('.h5'):\n",
    "                source_file_path = os.path.join(root, file_name)\n",
    "                dest_file_path = os.path.join(dest_dir, file_name)\n",
    "\n",
    "                # Compress and copy the HDF5 file\n",
    "                compress_and_copy_h5_file(source_file_path, dest_file_path, compression, compression_opts)\n",
    "\n",
    "def compress_and_copy_h5_file(original_file_path, new_file_path, compression='gzip', compression_opts=9):\n",
    "    with h5py.File(original_file_path, 'r') as original_file:\n",
    "        # Create a new HDF5 file\n",
    "        with h5py.File(new_file_path, 'w') as new_file:\n",
    "            # Copy each dataset from the original file to the new file\n",
    "            for dataset_name, dataset in original_file.items():\n",
    "                # Create a new dataset in the new file with compression\n",
    "                new_dataset = new_file.create_dataset(\n",
    "                    name=dataset_name,\n",
    "                    data=dataset[...],  # Use [...] to read the entire dataset\n",
    "                    compression=compression,\n",
    "                    compression_opts=compression_opts\n",
    "                )\n",
    "\n",
    "                # Copy attributes from the original dataset to the new dataset\n",
    "                for attr_name, attr_value in dataset.attrs.items():\n",
    "                    new_dataset.attrs[attr_name] = attr_value\n",
    "\n",
    "# Example usage\n",
    "source_directory = '/data02/gkim/stem_cell_jwshin/data/data_check_25d/big'\n",
    "\n",
    "# Specify compression settings for the new files\n",
    "compression_type = 'lzf'  # Choose compression algorithm: 'gzip', 'lzf', etc.\n",
    "compression_level = None  # Choose compression level (0-9)\n",
    "destination_directory = '/data02/gkim/stem_cell_jwshin/data/data_check_25d/comp_lzf'\n",
    "compress_and_copy_h5_files(source_directory, destination_directory, compression_type, compression_level)\n",
    "\n",
    "\n",
    "compression_type = 'gzip'  # Choose compression algorithm: 'gzip', 'lzf', etc.\n",
    "compression_level = 3  # Choose compression level (0-9)\n",
    "destination_directory = '/data02/gkim/stem_cell_jwshin/data/data_check_25d/comp_gzip3'\n",
    "compress_and_copy_h5_files(source_directory, destination_directory, compression_type, compression_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_type = 'gzip'  # Choose compression algorithm: 'gzip', 'lzf', etc.\n",
    "compression_level = 1  # Choose compression level (0-9)\n",
    "destination_directory = '/data02/gkim/stem_cell_jwshin/data/data_check_25d/comp_gzip1'\n",
    "compress_and_copy_h5_files(source_directory, destination_directory, compression_type, compression_level)\n",
    "\n",
    "compression_type = 'gzip'  # Choose compression algorithm: 'gzip', 'lzf', etc.\n",
    "compression_level = 2  # Choose compression level (0-9)\n",
    "destination_directory = '/data02/gkim/stem_cell_jwshin/data/data_check_25d/comp_gzip2'\n",
    "compress_and_copy_h5_files(source_directory, destination_directory, compression_type, compression_level)\n",
    "\n",
    "compression_type = 'gzip'  # Choose compression algorithm: 'gzip', 'lzf', etc.\n",
    "compression_level = 4  # Choose compression level (0-9)\n",
    "destination_directory = '/data02/gkim/stem_cell_jwshin/data/data_check_25d/comp_gzip4'\n",
    "compress_and_copy_h5_files(source_directory, destination_directory, compression_type, compression_level)\n",
    "\n",
    "compression_type = 'gzip'  # Choose compression algorithm: 'gzip', 'lzf', etc.\n",
    "compression_level = 5  # Choose compression level (0-9)\n",
    "destination_directory = '/data02/gkim/stem_cell_jwshin/data/data_check_25d/comp_gzip5'\n",
    "compress_and_copy_h5_files(source_directory, destination_directory, compression_type, compression_level)\n",
    "\n",
    "compression_type = 'gzip'  # Choose compression algorithm: 'gzip', 'lzf', etc.\n",
    "compression_level = 6  # Choose compression level (0-9)\n",
    "destination_directory = '/data02/gkim/stem_cell_jwshin/data/data_check_25d/comp_gzip6'\n",
    "compress_and_copy_h5_files(source_directory, destination_directory, compression_type, compression_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_type = 'gzip'  # Choose compression algorithm: 'gzip', 'lzf', etc.\n",
    "compression_level = 0  # Choose compression level (0-9)\n",
    "destination_directory = '/data02/gkim/stem_cell_jwshin/data/data_check_25d/comp_gzip0'\n",
    "compress_and_copy_h5_files(source_directory, destination_directory, compression_type, compression_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'big': 0}\n",
      "()\n",
      "Dataset Dir :  /data02/gkim/stem_cell_jwshin/data/data_check_25d len :  1\n",
      "{'comp_lzf': 0}\n",
      "()\n",
      "Dataset Dir :  /data02/gkim/stem_cell_jwshin/data/data_check_25d len :  1\n",
      "{'comp_gzip1': 0}\n",
      "()\n",
      "Dataset Dir :  /data02/gkim/stem_cell_jwshin/data/data_check_25d len :  1\n",
      "{'comp_gzip2': 0}\n",
      "()\n",
      "Dataset Dir :  /data02/gkim/stem_cell_jwshin/data/data_check_25d len :  1\n",
      "{'comp_gzip3': 0}\n",
      "()\n",
      "Dataset Dir :  /data02/gkim/stem_cell_jwshin/data/data_check_25d len :  1\n",
      "{'comp_gzip4': 0}\n",
      "()\n",
      "Dataset Dir :  /data02/gkim/stem_cell_jwshin/data/data_check_25d len :  1\n",
      "{'comp_gzip5': 0}\n",
      "()\n",
      "Dataset Dir :  /data02/gkim/stem_cell_jwshin/data/data_check_25d len :  1\n",
      "{'comp_gzip6': 0}\n",
      "()\n",
      "Dataset Dir :  /data02/gkim/stem_cell_jwshin/data/data_check_25d len :  1\n"
     ]
    }
   ],
   "source": [
    "# checking reading speed of data into GPUs\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import numpy as np\n",
    "# torch.backends.cudnn.benchmark = True\n",
    "\n",
    "from datas.TomoLoader import TomoLoader\n",
    "\n",
    "#import utils\n",
    "from datas.preprocess3d import TRAIN_AUGS_3D, TEST_AUGS_3D\n",
    "from datas.preprocess25d import TRAIN_AUGS_25D, TEST_AUGS_25D, TRAIN_AUGS_25D_v2, TRAIN_AUGS_25D_v3, TRAIN_AUGS_25D_v4, TEST_AUGS_25D_v4 # you have to put rand = 0 angle = 90 for preprocess3d\n",
    "from datas.preprocess2d import TRAIN_AUGS_2D, TEST_AUGS_2D\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "\n",
    "data_path = '/data02/gkim/stem_cell_jwshin/data/data_check_25d'\n",
    "\n",
    "\n",
    "loader_big_v4 = TomoLoader(data_path, batch_size=1,#32,#arg.batch_size,\n",
    "                        transform=TRAIN_AUGS_25D_v4, aug_rate=0,\n",
    "                        num_workers=1, shuffle=False, drop_last=False, pats_class = ['big'])\n",
    "                        \n",
    "\n",
    "loader_big_v4_lzf = TomoLoader(data_path, batch_size=1,#32,#arg.batch_size,\n",
    "                        transform=TRAIN_AUGS_25D_v4, aug_rate=0,\n",
    "                        num_workers=1, shuffle=False, drop_last=False, pats_class = ['comp_lzf'])\n",
    "                        \n",
    "\n",
    "\n",
    "loader_big_v4_gzip1 = TomoLoader(data_path, batch_size=1,#32,#arg.batch_size,\n",
    "                        transform=TRAIN_AUGS_25D_v4, aug_rate=0,\n",
    "                        num_workers=1, shuffle=False, drop_last=False, pats_class = ['comp_gzip1'])\n",
    "\n",
    "loader_big_v4_gzip2 = TomoLoader(data_path, batch_size=1,#32,#arg.batch_size,\n",
    "                        transform=TRAIN_AUGS_25D_v4, aug_rate=0,\n",
    "                        num_workers=1, shuffle=False, drop_last=False, pats_class = ['comp_gzip2'])\n",
    "\n",
    "loader_big_v4_gzip3 = TomoLoader(data_path, batch_size=1,#32,#arg.batch_size,\n",
    "                        transform=TRAIN_AUGS_25D_v4, aug_rate=0,\n",
    "                        num_workers=1, shuffle=False, drop_last=False, pats_class = ['comp_gzip3'])\n",
    "                        \n",
    "loader_big_v4_gzip4 = TomoLoader(data_path, batch_size=1,#32,#arg.batch_size,\n",
    "                        transform=TRAIN_AUGS_25D_v4, aug_rate=0,\n",
    "                        num_workers=1, shuffle=False, drop_last=False, pats_class = ['comp_gzip4'])\n",
    "\n",
    "loader_big_v4_gzip5 = TomoLoader(data_path, batch_size=1,#32,#arg.batch_size,\n",
    "                        transform=TRAIN_AUGS_25D_v4, aug_rate=0,\n",
    "                        num_workers=1, shuffle=False, drop_last=False, pats_class = ['comp_gzip5'])\n",
    "\n",
    "loader_big_v4_gzip6 = TomoLoader(data_path, batch_size=1,#32,#arg.batch_size,\n",
    "                        transform=TRAIN_AUGS_25D_v4, aug_rate=0,\n",
    "                        num_workers=1, shuffle=False, drop_last=False, pats_class = ['comp_gzip6'])\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'comp_gzip0': 0}\n",
      "()\n",
      "Dataset Dir :  /data02/gkim/stem_cell_jwshin/data/data_check_25d len :  1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loader_big_v4_gzip0 = TomoLoader(data_path, batch_size=1,#32,#arg.batch_size,\n",
    "                        transform=TRAIN_AUGS_25D_v4, aug_rate=0,\n",
    "                        num_workers=1, shuffle=False, drop_last=False, pats_class = ['comp_gzip0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg time for v4 reading of ('/data02/gkim/stem_cell_jwshin/data/data_check_25d/big/230425.203246.H9_24hour_treat.033.Group1.A1.S033.h5',): 15.75506\n",
      "avg time for v4 lzf reading of ('/data02/gkim/stem_cell_jwshin/data/data_check_25d/comp_lzf/230425.203246.H9_24hour_treat.033.Group1.A1.S033.h5',): 22.20950\n",
      "avg time for v4 gzip1 reading of ('/data02/gkim/stem_cell_jwshin/data/data_check_25d/comp_gzip1/230425.203246.H9_24hour_treat.033.Group1.A1.S033.h5',): 12.79529\n",
      "avg time for v4 gzip2 reading of ('/data02/gkim/stem_cell_jwshin/data/data_check_25d/comp_gzip2/230425.203246.H9_24hour_treat.033.Group1.A1.S033.h5',): 26.98241\n",
      "avg time for v4 gzip3 reading of ('/data02/gkim/stem_cell_jwshin/data/data_check_25d/comp_gzip3/230425.203246.H9_24hour_treat.033.Group1.A1.S033.h5',): 25.66168\n",
      "avg time for v4 gzip4 reading of ('/data02/gkim/stem_cell_jwshin/data/data_check_25d/comp_gzip4/230425.203246.H9_24hour_treat.033.Group1.A1.S033.h5',): 22.44103\n",
      "avg time for v4 gzip5 reading of ('/data02/gkim/stem_cell_jwshin/data/data_check_25d/comp_gzip5/230425.203246.H9_24hour_treat.033.Group1.A1.S033.h5',): 21.53044\n",
      "avg time for v4 gzip6 reading of ('/data02/gkim/stem_cell_jwshin/data/data_check_25d/comp_gzip6/230425.203246.H9_24hour_treat.033.Group1.A1.S033.h5',): 22.76941\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(0, epochs):\n",
    "    for i, (input_, target_, path) in enumerate(loader_big_v4):\n",
    "        pass\n",
    "end = time.time()\n",
    "readtime = end-start\n",
    "print(\"avg time for v4 reading of {0}: {1:0.5f}\".format(path, readtime/epochs))\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(0, epochs):\n",
    "    for i, (input_, target_, path) in enumerate(loader_big_v4_lzf):\n",
    "        pass\n",
    "end = time.time()\n",
    "readtime = end-start\n",
    "print(\"avg time for v4 lzf reading of {0}: {1:0.5f}\".format(path, readtime/epochs))\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(0, epochs):\n",
    "    for i, (input_, target_, path) in enumerate(loader_big_v4_gzip1):\n",
    "        pass\n",
    "end = time.time()\n",
    "readtime = end-start\n",
    "print(\"avg time for v4 gzip1 reading of {0}: {1:0.5f}\".format(path, readtime/epochs))\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(0, epochs):\n",
    "    for i, (input_, target_, path) in enumerate(loader_big_v4_gzip2):\n",
    "        pass\n",
    "end = time.time()\n",
    "readtime = end-start\n",
    "print(\"avg time for v4 gzip2 reading of {0}: {1:0.5f}\".format(path, readtime/epochs))\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(0, epochs):\n",
    "    for i, (input_, target_, path) in enumerate(loader_big_v4_gzip3):\n",
    "        pass\n",
    "end = time.time()\n",
    "readtime = end-start\n",
    "print(\"avg time for v4 gzip3 reading of {0}: {1:0.5f}\".format(path, readtime/epochs))\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(0, epochs):\n",
    "    for i, (input_, target_, path) in enumerate(loader_big_v4_gzip4):\n",
    "        pass\n",
    "end = time.time()\n",
    "readtime = end-start\n",
    "print(\"avg time for v4 gzip4 reading of {0}: {1:0.5f}\".format(path, readtime/epochs))\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(0, epochs):\n",
    "    for i, (input_, target_, path) in enumerate(loader_big_v4_gzip5):\n",
    "        pass\n",
    "end = time.time()\n",
    "readtime = end-start\n",
    "print(\"avg time for v4 gzip5 reading of {0}: {1:0.5f}\".format(path, readtime/epochs))\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(0, epochs):\n",
    "    for i, (input_, target_, path) in enumerate(loader_big_v4_gzip6):\n",
    "        pass\n",
    "end = time.time()\n",
    "readtime = end-start\n",
    "print(\"avg time for v4 gzip6 reading of {0}: {1:0.5f}\".format(path, readtime/epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch00010] training progress: 10.000%"
     ]
    }
   ],
   "source": [
    "epoch = 10\n",
    "i = 9\n",
    "train_loader = list(range(0,100))\n",
    "msg_train = \"[Epoch{0:05}] training progress: {1:0.3f}%\".format(epoch, (i+1)/len(train_loader)*100)\n",
    "print('\\r' + msg_train, end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg time for v4 gzip0 reading of ('/data02/gkim/stem_cell_jwshin/data/data_check_25d/comp_gzip0/230425.203246.H9_24hour_treat.033.Group1.A1.S033.h5',): 51.47322\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "for epoch in range(0, epochs):\n",
    "    for i, (input_, target_, path) in enumerate(loader_big_v4_gzip0):\n",
    "        pass\n",
    "end = time.time()\n",
    "readtime = end-start\n",
    "print(\"avg time for v4 gzip0 reading of {0}: {1:0.5f}\".format(path, readtime/epochs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_fish",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
