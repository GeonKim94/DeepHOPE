{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'02_ecto_06h': 0, '02_ecto_12h': 1, '02_ecto_24h': 2, '03_meso_06h': 3, '03_meso_12h': 4, '03_meso_24h': 5, '04_endo_06h': 6, '04_endo_12h': 7, '04_endo_24h': 8, '05_ctl': 9}\n",
      "('230427', '230713', '230714', '230425', 'H9', 'h9', 'JAX', 'jax')\n",
      "Dataset Dir :  /workspace01/gkim/stem_cell_jwshin/data/23_SEC1H5_wider_v3_allh_onRA//train/ len :  609\n",
      "{'02_ecto_06h': 0, '02_ecto_12h': 1, '02_ecto_24h': 2, '03_meso_06h': 3, '03_meso_12h': 4, '03_meso_24h': 5, '04_endo_06h': 6, '04_endo_12h': 7, '04_endo_24h': 8, '05_ctl': 9}\n",
      "('230427', '230713', '230714', '230425', 'H9', 'h9', 'JAX', 'jax')\n",
      "Dataset Dir :  /workspace01/gkim/stem_cell_jwshin/data/23_SEC1H5_wider_v3_allh_onRA//val/ len :  72\n",
      "{'02_ecto_06h': 0, '02_ecto_12h': 1, '02_ecto_24h': 2, '03_meso_06h': 3, '03_meso_12h': 4, '03_meso_24h': 5, '04_endo_06h': 6, '04_endo_12h': 7, '04_endo_24h': 8, '05_ctl': 9}\n",
      "('230427', '230713', '230714', '230425', 'H9', 'h9', 'JAX', 'jax')\n",
      "Dataset Dir :  /workspace01/gkim/stem_cell_jwshin/data/23_SEC1H5_wider_v3_allh_onRA//test/ len :  71\n",
      "0\n",
      "Not Load\n",
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from vit_pytorch import SimpleViT\n",
    "from vit_pytorch.extractor import Extractor\n",
    "from datas.TomoLoader import TomoLoader\n",
    "from runners.clsRunner import clsRunner\n",
    "import datas.preprocess3d as pp3d # TRAIN_AUGS_3D, TEST_AUGS_3D, crop_shape, size_z\n",
    "import datas.preprocess25d as pp25d # #TRAIN_AUGS_25D, TEST_AUGS_25D, TRAIN_AUGS_25D_v4, TEST_AUGS_25D_v4\n",
    "import datas.preprocess2d as pp2d # TRAIN_AUGS_2D, TEST_AUGS_2D\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "torch_device = torch.device(\"cuda\")\n",
    "\n",
    "image_size = 3072#4096\n",
    "depth_z = 12\n",
    "patch_size = 384#256\n",
    "num_classes = 10\n",
    "pats_exclude = ('230427',)+('230713',)+('230714',)+('230425',)+('H9',)+('h9',)+('JAX',)+('jax',)\n",
    "pats_class = ('02_ecto_06h',)+('02_ecto_12',)+('02_ecto_24h',)+('03_meso_06h',)+('03_meso_12',)+('03_meso_24h',)+('04_endo_06h',)+('04_endo_12',)+('04_endo_24h',)+('05_ctl',)\n",
    "mode_class = 0\n",
    "reset_class = False\n",
    "w_metric = (10., 10., 10., 10., 10., 10., 10., 10., 10., 1., )\n",
    "\n",
    "pad25d = lambda img: pp25d.pad_25d(img, (image_size, image_size))\n",
    "cencrop25d = lambda img: pp25d.center_crop_25d_alt(img, (image_size,image_size,depth_z))\n",
    "tform_test = [\n",
    "    pad25d,\n",
    "    cencrop25d,\n",
    "    pp25d.calibration,\n",
    "    pp25d.channel_fromz,\n",
    "    pp25d.to_tensor\n",
    "]\n",
    "\n",
    "randcrop25d = lambda img: pp25d.random_crop_25d_alt(img, (image_size,image_size,depth_z))\n",
    "tform_train = [\n",
    "    randcrop25d,\n",
    "    pp25d.flipud_3d,\n",
    "    pp25d.fliplr_3d,\n",
    "    pp25d.swapaxes_3d,\n",
    "    pp25d.calibration,\n",
    "    pp25d.gaussian_3d,\n",
    "    pp25d.channel_fromz,\n",
    "    pp25d.to_tensor,\n",
    "]\n",
    "    \n",
    "\n",
    "path_data = '/workspace01/gkim/stem_cell_jwshin/data/23_SEC1H5_wider_v3_allh_onRA/'\n",
    "train_loader = TomoLoader(path_data+'/train/', 1, \n",
    "                            transform=tform_train, aug_rate=0,\n",
    "                            num_workers=0, shuffle=False, drop_last=True,\n",
    "                            pats_exclude = pats_exclude,pats_class = pats_class,\n",
    "                            reset_class = reset_class, mode_class = mode_class)\n",
    "val_loader = TomoLoader(path_data+'/val/', 1, \n",
    "                            transform=tform_train, aug_rate=0,\n",
    "                            num_workers=0, shuffle=False, drop_last=True,\n",
    "                            pats_exclude = pats_exclude,pats_class = pats_class,\n",
    "                            reset_class = reset_class, mode_class = mode_class)\n",
    "test_loader = TomoLoader(path_data+'/test/', 1, \n",
    "                            transform=tform_train, aug_rate=0,\n",
    "                            num_workers=0, shuffle=False, drop_last=True,\n",
    "                            pats_exclude = pats_exclude,pats_class = pats_class,\n",
    "                            reset_class = reset_class, mode_class = mode_class)\n",
    "\n",
    "\n",
    "net = SimpleViT(\n",
    "    image_size = image_size,\n",
    "    patch_size = patch_size,\n",
    "    num_classes = num_classes,\n",
    "    dim = 1024,\n",
    "    depth = 3,#6 for normal\n",
    "    heads = 1,\n",
    "    mlp_dim = 1024,\n",
    "    channels = 12\n",
    ")\n",
    "model_type = 'SimpleViT'\n",
    "epoch = 1000\n",
    "save_dir = '/workspace01/gkim/stem_cell_jwshin/outs/240718_VIT_3072_384'\n",
    "result_dir = '/workspace01/gkim/stem_cell_jwshin/outs/240718_VIT_3072_384'\n",
    "\n",
    "net = net.to(torch_device)\n",
    "loss = torch.nn.CrossEntropyLoss(weight = torch.Tensor(w_metric).to(torch_device))\n",
    "optim = torch.optim.Adam(net.parameters())\n",
    "model = clsRunner(net, optim, torch_device, loss,#logger = logger,\n",
    "                    model_type = model_type, epoch = epoch, save_dir = save_dir, result_dir = result_dir, \n",
    "                    w_metric = w_metric, w_metric_test = w_metric, w_metric_train = w_metric, n_class = num_classes)\n",
    "\n",
    "#import pdb; pdb.set_trace()\n",
    "img, target, path = next(iter(train_loader))\n",
    "print(net(img.to(torch_device)).shape)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_RI2FL_github",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
