{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.io\n",
    "import itertools\n",
    "\n",
    "from models.AE3D import AE3D,ae_loss,mse_loss\n",
    "import datas.preprocess3d\n",
    "from datas.Patch3DLoader import Patch3DLoader\n",
    "#from datas.preprocess3d import TRAIN_AUGS_3D, TEST_AUGS_3D, TRAIN_NOAUGS_3D\n",
    "\n",
    "\n",
    "class Loss_PCC(torch.nn.Module): ## works properly for isotropic 2D & if 3D it's summed over all z\n",
    "    def __init__(self, eps = 1e-8, torch_device=None):\n",
    "        super(Loss_PCC, self).__init__()\n",
    "        self.torch_device = torch_device\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        tup_dim = tuple([i for i in range(2,len(img1.shape))])\n",
    "        mu1 = torch.mean(img1, tup_dim)\n",
    "        mu2 = torch.mean(img2, tup_dim)\n",
    "        sigma1 = torch.std(img1, tup_dim)\n",
    "        sigma2 = torch.std(img2, tup_dim)\n",
    "        \n",
    "        for i in range(2,len(img1.shape)):\n",
    "            mu1 = mu1.unsqueeze(i)\n",
    "            mu2 = mu2.unsqueeze(i)\n",
    "            sigma1 = sigma1.unsqueeze(i)\n",
    "            sigma2 = sigma2.unsqueeze(i)\n",
    "\n",
    "        mu1 = mu1.repeat(1,1,*img1.shape[2:])\n",
    "        mu2 = mu2.repeat(1,1,*img1.shape[2:])\n",
    "        sigma1 = sigma1.repeat(1,1,*img1.shape[2:])\n",
    "        sigma2 = sigma2.repeat(1,1,*img1.shape[2:])\n",
    "\n",
    "        img1_ = (img1-mu1)/(sigma1+self.eps)\n",
    "        img2_ = (img2-mu2)/(sigma2+self.eps)\n",
    "        \n",
    "        PCC = img1_*img2_\n",
    "        return 1-PCC.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = AE3D(input_channels=1, latent_dim=16384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '6'\n",
    "torch_device = torch.device(\"cuda\")\n",
    "\n",
    "#net = nn.DataParallel(net.to(torch_device))\n",
    "net = net.to(torch_device) # for single GPU\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "pcc_loss = Loss_PCC(eps = 1e-6, torch_device = torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/data02/gkim/stem_cell_jwshin/data/230811+230502_3DH5_wider_v3_allh_onRA/' # size_z = 32 in preprocess3d \n",
    "# data_path = '/data02/gkim/stem_cell_jwshin/data/230811+230502_3DH5_wider_v3_allh_onRA/' # size_z = 12 in preprocess3d \n",
    "train_loader = Patch3DLoader(data_path + \"/train\", 4,\n",
    "                                transform=datas.preprocess3d.TRAIN_AUGS_3D, aug_rate=0.0,\n",
    "                                num_workers=4, shuffle=False, drop_last=False)\n",
    "enum_train = itertools.cycle(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for ii in range(0, 300):\n",
    "        #train\n",
    "    net.train()\n",
    "    print('starting to train epoch[%05d]' % ii)\n",
    "    batch_current = 0\n",
    "    for (input_, target_, path) in enum_train:\n",
    "        input_, target_ = input_.to(torch_device), target_.to(torch_device)\n",
    "        optimizer.zero_grad()\n",
    "        recon, z = net(input_)\n",
    "        mse = mse_loss(recon, input_)\n",
    "        pcc = pcc_loss(recon, input_)\n",
    "        \n",
    "        loss = 0.01*mse+1.0*pcc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_current = batch_current+1\n",
    "        # print('\\r')\n",
    "        # print(\"training: epoch %05d batch %d/%d, pcc loss = %.3f, mse loss = %.3f\"\n",
    "        #       % (ii, batch_current, len(test_loader), pcc, mse))\n",
    "        if batch_current == len(train_loader):\n",
    "            break\n",
    "\n",
    "    if ii%1 == 0:\n",
    "        str_print = f\"epoch number {ii}: pcc loss = {pcc}, mse loss = {mse}\"\n",
    "        print(str_print)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/data02/gkim/stem_cell_jwshin/data/230811+230502_3DH5_wider_v3_allh_onRA/'\n",
    "test_loader = Patch3DLoader(data_path + \"/test\", 1,\n",
    "                                transform=datas.preprocess3d.TEST_AUGS_3D, aug_rate=0.0,\n",
    "                                num_workers=1, shuffle=False, drop_last=False)\n",
    "enum_test = itertools.cycle(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(input_, target_, path) = next(enum_test)\n",
    "\n",
    "\n",
    "input_, target_ = input_.to(torch_device), target_.to(torch_device)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "recon, z = net(input_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_batch = 0\n",
    "idx_z = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.squeeze(input_[0].detach().cpu().numpy(), axis = 0)[:,:,idx_z].shape\n",
    "plt.imshow(np.squeeze(input_[idx_batch].detach().cpu().numpy(), axis = 0)[:,:,idx_z], vmin = 0, vmax = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.squeeze(recon[idx_batch].detach().cpu().numpy(), axis = 0)[:,:,idx_z], vmin = 0, vmax = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_clam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
