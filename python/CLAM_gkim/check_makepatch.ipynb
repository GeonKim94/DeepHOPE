{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "from models.VAE3D import VAE3D,vae_loss,mse_loss,kld_loss\n",
    "# torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# example for mnist\n",
    "from datas.Patch3DLoader import Patch3DLoader\n",
    "\n",
    "import utils\n",
    "from datas.preprocess3d import TRAIN_AUGS_3D, TEST_AUGS_3D, TRAIN_AUGS_25D, TEST_AUGS_25D\n",
    "\n",
    "from Logger import Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Loss_PCC(torch.nn.Module): ## works properly for isotropic 2D & if 3D it's summed over all z\n",
    "    def __init__(self, eps = 1e-8, torch_device=None):\n",
    "        super(Loss_PCC, self).__init__()\n",
    "        self.torch_device = torch_device\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        tup_dim = tuple([i for i in range(2,len(img1.shape))])\n",
    "        mu1 = torch.mean(img1, tup_dim)\n",
    "        mu2 = torch.mean(img2, tup_dim)\n",
    "        sigma1 = torch.std(img1, tup_dim)\n",
    "        sigma2 = torch.std(img2, tup_dim)\n",
    "        \n",
    "        for i in range(2,len(img1.shape)):\n",
    "            mu1 = mu1.unsqueeze(i)\n",
    "            mu2 = mu2.unsqueeze(i)\n",
    "            sigma1 = sigma1.unsqueeze(i)\n",
    "            sigma2 = sigma2.unsqueeze(i)\n",
    "\n",
    "        mu1 = mu1.repeat(1,1,*img1.shape[2:])\n",
    "        mu2 = mu2.repeat(1,1,*img1.shape[2:])\n",
    "        sigma1 = sigma1.repeat(1,1,*img1.shape[2:])\n",
    "        sigma2 = sigma2.repeat(1,1,*img1.shape[2:])\n",
    "\n",
    "        img1_ = (img1-mu1)/(sigma1+self.eps)\n",
    "        img2_ = (img2-mu2)/(sigma2+self.eps)\n",
    "        \n",
    "        PCC = img1_*img2_\n",
    "        return 1-PCC.mean()\n",
    "pcc_loss = Loss_PCC(eps = 1e-6, torch_device = torch_device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample usage\n",
    "path_input = '/data02/gkim/stem_cell_jwshin/data/230502_TCF/00_train/H9_untreated/230425.142330.H9_untreated.003.Group1.A1.S003/230425.142330.H9_untreated.003.Group1.A1.S003.TCF'\n",
    "input_file = h5py.File(path_input, 'r') # 220801 for -v7.3 mat files\n",
    "input_data = input_file['/Data/3D/000000']\n",
    "input_data = input_data[17:49,872:1256,872:1256].astype(np.float16)/10000\n",
    "\n",
    "cap_min = 1.33\n",
    "cap_max = 1.4\n",
    "input_data = (input_data - cap_min)/(cap_max-cap_min)\n",
    "input_data[input_data>1.0] = 1.0\n",
    "\n",
    "input_data = np.transpose(input_data, (2,1,0))\n",
    "input_data = np.expand_dims(input_data, axis=0)\n",
    "input_data = np.expand_dims(input_data, axis=0)\n",
    "input_data = torch.Tensor(input_data)\n",
    "print(input_data.shape)\n",
    "\n",
    "input_data = input_data.to(torch_device)\n",
    "#input_data = torch.rand(3, 1, 384, 384, 32).to(torch_device)  # Replace with your actual data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range(0,10000):\n",
    "    #import pdb; pdb.set_trace()\n",
    "    optimizer.zero_grad()\n",
    "    recon, mu, logvar = vae(input_data)\n",
    "    #loss = vae_loss(recon, input_data, mu, logvar).to(torch_device)\n",
    "    mse = mse_loss(recon, input_data)\n",
    "    kld = kld_loss(mu, logvar)\n",
    "    pcc = pcc_loss(recon, input_data)\n",
    "    if ii%100 == 0:\n",
    "        str_print = f\"epoch number {ii}: MSE= {mse}, PCC = {pcc}, KLD = {kld}\"\n",
    "        print(str_print)\n",
    "    loss = mse+kld+pcc\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_save = recon.detach().cpu().numpy()\n",
    "recon_save = np.squeeze(recon_save)\n",
    "input_save = input_data.cpu().numpy()\n",
    "input_save = np.squeeze(input_save)\n",
    "\n",
    "path_output = '/data02/gkim/stem_cell_jwshin/230425.142330.H9_untreated.003.Group1.A1.S003_VAEout_v7.h5'\n",
    "output_file = h5py.File(path_output, 'w') #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_save.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "output_file.create_dataset('output',\n",
    "                           shape = recon_save.shape,\n",
    "                           maxshape = (None, None, None),\n",
    "                           data = recon_save)\n",
    "#output_file['output'][...] = recon_save\n",
    "output_file.create_dataset('input',\n",
    "                           shape = input_save.shape,\n",
    "                           maxshape = (None, None, None),\n",
    "                           data=input_save)\n",
    "#output_file['input'][...] = input_save\n",
    "output_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_RI2FL_github",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
